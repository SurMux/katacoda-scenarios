Daten gibt es zur heutigen Zeit überall in Massen. Sie werden standardmäßig durch unterschiedlichen Quellenst gesammelt und standardmäßig in riesigen Datenbanken gesammelt und bei Bedarf zur weiteren Verarbeitung ausgelesen.

Je mehr Daten anfallen, desto zeitintensiver gestaltet sich dieser Prozess. Wenn die Daten nun in Echtzeit benötigt werden, kann es mit dem Standard Vorgehen zu Komplikationen führen.

In diesem Bereich klinkt sich Apache Kafka ein. Als eine Event-Streaming-Plattform unterstützt Kafka das Senden, speichern und Auslesen von Daten in Echtzeit. 
Bei Bedarf können die Datenströme sogar in Echtzeit abgerufen, verarbeitet und an weitere Empfänger versendet werden.
Durch die Aufteilungsmöglichkeiten der einzelnen Datenspeicher, kann ein riesige ineffiziente Datenbank umgangen werden.

Dieses Katacoda Szenario erzielt die Einrichtung eines Kafka Systems und eine Darstellung der Datenströme vom Senden der Daten über die Speicherung bis zum Auslesen der Daten.