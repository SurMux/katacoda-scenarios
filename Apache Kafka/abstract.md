Daten gibt es zur heutigen Zeit überall in Massen. Sie werden standardmäßig durch unterschiedliche Quellen gesammelt, in riesigen Datenbanken gelagert und bei Bedarf zur weiteren Verarbeitung ausgelesen.

Je mehr Daten anfallen, desto zeitintensiver gestaltet sich dieser Prozess. Wenn die Daten nun in Echtzeit benötigt werden, kann es mit dem Standardvorgehen zu Komplikationen führen.

In diesem Bereich klinkt sich Apache Kafka ein. Als eine Event-Streaming-Plattform unterstützt Kafka das Senden, Speichern und Auslesen von Daten in Echtzeit. 
Bei Bedarf können die Datenströme sogar in Echtzeit abgerufen, verarbeitet und an weitere Empfänger versendet werden.
Durch die Aufteilungsmöglichkeiten der einzelnen Datenspeicher, kann eine riesige ineffiziente Datenbank umgangen werden.

Dieses Katacoda Szenario erzielt die Einrichtung eines Kafka Systems und eine Darstellung der Datenströme vom Senden der Daten über die Speicherung bis zum Auslesen der Daten.